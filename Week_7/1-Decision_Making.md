---
authors: goodman
---

# Decision making

[Download the slides here](slides/W7-V0-decision-making.pptx)

:::{iframe} https://www.youtube.com/embed/n_05hOV2hS0
:width: 100%
:::
---

```{danger} Work in progress
The text below has been transcribed by hand from the video above but has not yet been reviewed. Please use the videos and slides as the primary material and the text as support until I have a chance to proofread everything. When I have done this, I will remove this message.
```

## Introduction

This week‚Äôs sections don‚Äôt cover a single theme. Instead, we‚Äôll look at a few different strands of work in neuroscience that could be of interest to people with a machine learning or quantitative background. We may also add more sections in future years.

In this section, we‚Äôll focus on decision making - a really broad topic of course, because in a way you can think of everything the brain has to do is about making decisions.

:::{note}
We're going to focus on one particular aspect of decision making that has been studied a lot in neuroscience and less in machine learning: Reaction times. 

That is, how long does it take to make a decision based on a stream of information arriving continuously over time.
:::

## Two-alternative forced choice (2AFC)

Let‚Äôs make this even more concrete with a very specific sort of task, the two-alternative forced choice, or 2AFC.

In [this task](#2afc), participants are shown some sort of image or movie and asked to decide between two options. A common one is the random dot kinematogram, with random dots on the screen some of which are moving coherently either to the left or right, and some moving randomly. The participants have to determine which direction the coherent dots are moving, and sometimes they‚Äôre asked to make their decision as quickly as possible. The reaction time is the time between the start of the video and the moment when they press the button.

```{figure} figures/taskreact.png
:label: 2afc
:alt: Random Dot Kinematogram
:align: center
:width: 500px

Random Dot Kinematogram Experiment
```
If you run this experiment, you see characteristic skewed distributions of reaction times. [This data](#2afcresult) is actually from a slightly different task where participants were asked whether or not they had seen the image being shown before or not.

```{figure} figures/taskresult.png
:label: 2afcresult
:alt: 2AFC Experiment Results
:align: center

2AFC Experiment Results {cite:p}`https://doi.org/10.1037/0033-295X.85.2.59`.
```

## Drift diffusion / random walk model

There‚Äôs a beautifully simple theory for how we make these decisions which can account for these reaction times:

* Imagine that as time goes by, sometimes a bit of evidence arrives that is on its own unreliable, but suggestive that the dots are moving right rather than left. Then another, followed by one that suggests that left is more likely, and so on. We keep track of a running total of how much evidence we‚Äôve received that suggests right versus left, and once it crosses some threshold we make our decision. This is a biased random walk, where it‚Äôs more likely we take a step in the correct direction than the wrong direction.

<!-- ```{figure} 10.png
:label: 10res
:alt: Biased Random Walk With 10 Time Points
:align: center

Biased Random Walk With 10 Time Points
```
-->
(10res)=
![](#10-graph)

* As we increase the number of time points from [10](#10res) to [30](#30res).

<!-- ```{figure} 30.png
:label: 30res
:alt: Biased Random Walk With 30 Time Points
:align: center

Biased Random Walk With 30 Time Points
```
-->

(30res)=
![](#30-graph)

* Then from [30](#30res) to [100](#100res).

<!-- ```{figure} 100.png
:label: 100res
:alt: Biased Random Walk With 100 Time Points
:align: center

Biased Random Walk With 100 Time Points
```
-->

(100res)=
![](#100-graph)

And all the way up to [1000](#1000res), this random walk looks more and more like Brownian motion with a drift. This gives us a mathematical theory that lets us analytically compute expressions for the reaction time distributions.

<!-- ```{figure} 1000.png
:label: 1000res
:alt: Biased Random Walk With 1000 Time Points
:align: center

Biased Random Walk With 1000 Time Points
```
-->

(1000res)=
![](#1000-graph)

* Or we can just numerically plot them, [here](#highlow) with either a low decision threshold or a high decision threshold. 

<!-- ```{figure} distr.png
:label: highlow
:alt: Plot of Reaction Time Distributions for High and Low Decision Thresholds
:align: center

Plot of Reaction Time Distributions for High and Low Decision Thresholds
```
-->

(highlow)=
![](#highlow-graph)

* And sure enough, we get something that looks very much like the data from [experiments](#2afcresult).

## Probabilistic interpretation

That‚Äôs nice, but still this model might seem a bit ad hoc. Fortunately, it turns out that there‚Äôs a neat probabilistic interpretation of this model.

* Let‚Äôs set up a probabilistic model of the task. We‚Äôll say there‚Äôs a true direction $D$ that can be either $L$ or $R$, and both options are equally likely. You can actually modify this to make the options have different probabilities too.

```{math}
\begin{gather}
    \text{True direction } ùê∑=ùêø \text{ or } ùê∑=ùëÖ \\
    \text{ equally likely } \textcolor{#fcad03}{ùëÉ(ùê∑=ùêø)}=\textcolor{#fcad03}{ùëÉ(ùê∑=ùëÖ)}=1/2
\end{gather}
```

* Now we have the data observed at time t is a series of symbols $ùëã_ùë°$ each of which is $R$ or $L$.

```{math}
    \text{Observed data } ùëã_ùë°=(ùëÖ,ùëÖ,ùêø,ùëÖ,ùêø,ùëÖ,ùëÖ,ùëÖ,‚Ä¶)
```

* The probability you observe the correct symbol is $p$ and the probability you observe the incorrect symbol $1-p$.

```{math}
\begin{gather}
    \text{Probabilities } ùëÉ(ùëã_ùë°=ùê∑)= \textcolor{#03bafc}{ùëù} \text{ and } ùëÉ(ùëã_ùë°=‚àíùê∑)=1‚àí \textcolor{#03bafc}{ùëù} \\
    \text{where } \textcolor{#03bafc}{ùëù} > 1 ‚àí \textcolor{#03bafc}{ùëù}
\end{gather}
```

* Now given we know the observed data and we want to infer the unknown value of $D$, we compute which of the two options is more likely. We‚Äôll write that as $\textcolor{#a503fc}{r(X)}$ equals the ratio of the probability that $D=R$ given the observations $X$, divided by the probability that $D=L$. If this ratio is high then $D=R$ is much more likely, and if it‚Äôs low then $D=L$ is more likely.

```{math}
\begin{gather}
    \text{Write } \textcolor{#a503fc}{r(X)}=\frac{ùëÉ(ùê∑=ùëÖ|ùëø)}{ùëÉ(ùê∑=ùêø|ùëø)} \text{ the likelihood ratio.} \\
    \text{If this is high then } ùê∑=ùëÖ \text{ is more likely.}
\end{gather}
```

* We use Bayes‚Äô theorem to rewrite the probability that $D=R$ given X in terms of the probability of $X$ given $D=R$ times the probability $D=R$ over the probability of $X$. And the same thing for the probability that $D=L$ on the bottom.

```{math}
    \textcolor{#a503fc}{r(X)}=\frac{ùëÉ(ùëø|ùê∑=ùëÖ)_{\textcolor{#fcad03}{ùëÉ(ùê∑=R)}/\textcolor{#128f04}{P(X)}}}{ùëÉ(ùëø|ùê∑=L)_{\textcolor{#fcad03}{ùëÉ(ùê∑=ùêø)}/\textcolor{#128f04}{P(X)}}}
```

* The $\textcolor{#128f04}{P(X)}$ cancels and both the prior probabilities $D=R$ and $D=L$ are both ¬Ω so they cancel too.

```{math}
    \textcolor{#a503fc}{r(X)}=\frac{ùëÉ(ùëø|ùê∑=ùëÖ)}{ùëÉ(ùëø|ùê∑=L)}
```

* The observations at different time points are independent, so we can expand these as a product of the probabilities at each time point.

```{math}
    \textcolor{#a503fc}{r(X)}=\frac{\Pi_{t}ùëÉ(ùëø_t|ùê∑=ùëÖ)}{\Pi_{t}ùëÉ(ùëø_t|ùê∑=L)}
```

* And this product is the same at the top and bottom so we can pull it out.

```{math}
    \textcolor{#a503fc}{r(X)}=\Pi_{t}\frac{ùëÉ(ùëø_t|ùê∑=ùëÖ)}{ùëÉ(ùëø_t|ùê∑=L)}
```

* Now to see what‚Äôs going on more clearly we take the log of this ratio to get the log likelihood ratio.

```{math}
    log\textcolor{#a503fc}{r(X)}=log\Pi_{t}\frac{ùëÉ(ùëø_t|ùê∑=ùëÖ)}{ùëÉ(ùëø_t|ùê∑=L)}
```    

* The log of a product is the sum of the logs.

```{math}
    log\textcolor{#a503fc}{r(X)}=\Sigma_{t}log\frac{ùëÉ(ùëø_t|ùê∑=ùëÖ)}{ùëÉ(ùëø_t|ùê∑=L)}
```

* And we‚Äôll write the individual terms as the ‚Äúevidence at time $t$‚Äù $\epsilon(ùëã_ùë°)$.

```{math}
     log\textcolor{#a503fc}{r(X)}=\Sigma_{t}\epsilon(ùëã_ùë°)
```

* This evidence will be log of $\textcolor{#03bafc}{ùëù}/ 1- \textcolor{#03bafc}{ùëù}$ when $ùëã_ùë°=ùêø$. If $ùëã_ùë°=ùëÖ$ it‚Äôs log of $(1- \textcolor{#03bafc}{ùëù})/ \textcolor{#03bafc}{ùëù}$ but this is just negative log of $\textcolor{#03bafc}{ùëù} / 1- \textcolor{#03bafc}{ùëù}$.

```{math}
\begin{gather}
    \epsilon(ùëã_ùë°) = log\frac{\textcolor{#03bafc}{ùëù}}{1-\textcolor{#03bafc}{ùëù}} \text{ if } ùëã_ùë° = L \\
    \text{or } -log\frac{\textcolor{#03bafc}{ùëù}}{1-\textcolor{#03bafc}{ùëù}} \text{ if } ùëã_ùë° = R
\end{gather}
```

* With that we can write the log likelihood ratio as a constant term multiplied by the sum of terms $\textcolor{#fc0505}{\Sigma_{t}\delta(ùëã_ùë°)}$ which are $+1$ if $ùëã_ùë°=ùëÖ$ and $-1$ if $ùëã_ùë°=ùêø$.

```{math}
    logùëü(ùëø)=log\frac{\textcolor{#03bafc}{ùëù}}{1-\textcolor{#03bafc}{ùëù}} * \textcolor{#fc0505}{\Sigma_{t}\delta(ùëã_ùë°)}
```

* But this sum is precisely the random walk we saw previously.

```{math}
    \textcolor{#fc0505}{\Sigma_{t}\delta(ùëã_ùë°)} \text{ is random walk!}
```

* When D=R the sum increases by 1 with probability $\textcolor{#03bafc}{ùëù}$ and decreases by 1 with probability $1-\textcolor{#03bafc}{ùëù}$, and vice versa if $D=L$.

* We can understand the decision threshold in terms of probability now. We wait until the log likelihood ratio is bigger than some threshold $\theta$ or equivalently that the likelihood ratio is bigger than $ùëí^{\theta}$.

```{math}
\begin{gather}
    \text{Decision threshold }\theta \text{: } ùê∑=ùëÖ \text{ more likely if} \\
    log\textcolor{#a503fc}{r(X)}>\theta \text{ or } \textcolor{#a503fc}{r(X)}>ùëí^\theta
\end{gather}
```

* And this happens when the sum of the deltas is bigger than some threshold, precisely as in the drift diffusion model.

```{math}
    \text{This is true if } \textcolor{#fc0505}{\Sigma_{t}\delta(ùëã_ùë°)} >  \text{const} = \theta / log\frac{\textcolor{#03bafc}{ùëù}}{1-\textcolor{#03bafc}{ùëù}}
```

<!-- ```{figure} probinter.png
:label: probi
:alt: Probabilistic Interpretation of Model
:align: center

Probabilistic Interpretation of Model
```
-->

## Electrophysiological data

And finally, perhaps the best thing about this theory is that having come up with the model based on fitting behavioural observations and then finding a rigorous probabilistic interpretation, electrophysiological [experiments](#electrophys) find traces of these evidence accumulation processes across multiple different brain regions in multiple species.

```{figure} figures/electro.jpg
:label: electrophys
:alt: Experimental Evidence of Accumulation Processes Across Multiple Brain Regions and Species
:align: center

Experimental Evidence of Accumulation Processes Across Multiple Brain Regions and Species {cite:p}`https://doi.org/10.1016/j.tins.2018.06.005`.
```

Of course, it‚Äôs never quite as clean cut as the theory and all sorts of modifications have been proposed, like adaptive thresholds that get closer to the origin over time to represent the increasing urgency of making some sort of decision, or thresholds that adapt to wider context in various ways. There are also much more comprehensive Bayesian theories of decision making in which this model is just one special case, and so on. 

:::{seealso} For more
:class: dropdown
If you‚Äôre interested in reading more, there are some suggested starting points

* [Ratcliff (1978) "A theory of memory retrieval"](https://doi.org/10.1037/0033-295X.85.2.59)
* [Gold and Shadlen (2007) "The Neural Basis of Decision Making"](https://doi.org/10.1146/annurev.neuro.29.051605.113038)
* [O'Connell et al. (2018) "Bridging Neural and Computational Viewpoints on Perceptual Decision-Making"](https://doi.org/10.1016/j.tins.2018.06.005)
:::

:::{seealso} That's it!
In¬†the next section, we‚Äôll talk about Neuromorphic Computing.
:::